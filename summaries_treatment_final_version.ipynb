{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b686ee",
   "metadata": {},
   "source": [
    "# Corenplot Summaries treatment\n",
    "\n",
    "The aim of this script is to extract the lexic (words) used in each summaries associated to film characters. It will further be used to analyze the lexicon associated to each gender.\n",
    "\n",
    "The Corenplot dataset is composed of 42'306 summaries in XML format, each of them have already been processed with a NLP (Stanford NLP). \n",
    "\n",
    "The following informations are retrieved from the summaries:\n",
    "- The sentence composing the summaries\n",
    "- The words dependencies, id est, the grammatical architecture of a single sentence, thus the association, in form of governor/dependent, of the words.\n",
    "- The coreference, id est, the links between words of different sentence\n",
    "\n",
    "The following script can be categorized as \"messy\" due to poor knowledge of the author in modules offering capacity to read xml files. The script is composed of various function to increase readablity, the main part is a the bottom and consist of a simple loop on all the summaries, aka, all the xml files. The script lack of optimization due to time restriction and thus needs approximately 11 hours to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aaef9c",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb9f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import lxml \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import xmltodict \n",
    "import pprint\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb33ce",
   "metadata": {},
   "source": [
    "### Coreference treatment\n",
    "\n",
    "**Goal**: Create a pandas dataframe containing in numeric style each coreference, thus the sentence number and the word position number for each governor word associated to a dependent word. In other words, it is a relation table which contain linked words.\n",
    "\n",
    "**How**: The function first loop on the coreference tag in the xml file, for each coreference tag, one representative word with multiple dependent words. The governor/representative word is a subject gramatically speaking, for example, it allows to find to who is attributed a certain pronouns: \"she\" in the 5th sentence is associated to \"Marie\" in 2nd sentence. The benefit from this analysis is to obtain character name (and furtherdown the gender) of sentences where no explicit names are used.\n",
    "\n",
    "**Comment**: This function is associated to the link_words function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140e62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreference_treatment(Bs_data):\n",
    "    \n",
    "    all_link = pd.DataFrame(columns=[\"representative_sentence\",\"representative_head\",\"dep_sentence\",\"dep_head\"],dtype='object')\n",
    "    \n",
    "    #select the bottom part of the xml file => coreference which represent link between sentences (var = pointer)\n",
    "    coreference = Bs_data.find_all('coreference')\n",
    "\n",
    "    #loop on each coreference, which is made of multiple sentence, one representative and one or more dependant sentences\n",
    "    for index in np.arange(1,len(coreference)):\n",
    "    \n",
    "        #select a specific coreference [index]\n",
    "        linked_sentence = np.array(coreference[index].find_all(\"sentence\"))\n",
    "        #the representative sentence is alwais the first sentence. The representative normally contain the character name (or subject)\n",
    "        representative = linked_sentence[0]\n",
    "        #all the other sentences from 1 coreference are the dependant\n",
    "        dep = linked_sentence[1:]\n",
    "    \n",
    "        #the heads represent the word of each sentence that have a relation: ex. \"representative_head\" = Arthur & dep_head = \"him\"\n",
    "        linked_sentence_head = np.array(coreference[index].find_all(\"head\"))\n",
    "        rep_head=linked_sentence_head[0]\n",
    "        dep_head=linked_sentence_head[1:]\n",
    "    \n",
    "        #concatenation of dependant sentence with dependant head\n",
    "        temp = np.concatenate([dep,dep_head],axis=1)\n",
    "    \n",
    "        #concatenante each dependant sentence + head to a single representant sentence+head  \n",
    "        temp2 = np.concatenate([np.full([dep.shape[0],1],representative),np.full([dep.shape[0],1],rep_head)],axis=1)\n",
    "    \n",
    "        #concatenate all together\n",
    "        temp = np.concatenate([temp2,temp],axis=1)\n",
    "    \n",
    "\n",
    "        #transform in DataFrame for simpliciti\n",
    "        temp = pd.DataFrame(data=temp,columns=[\"representative_sentence\",\"representative_head\",\"dep_sentence\",\"dep_head\"])\n",
    "        #save result\n",
    "        all_link = all_link.append(temp,ignore_index = True)\n",
    "    \n",
    "    return all_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05575dc7",
   "metadata": {},
   "source": [
    "### Dependence treatment\n",
    "\n",
    "**Goal**: As mentionned above, the dependence are the words connection from a single sentence. The aim of this function is to create a dataframe containing each words association in relation table. It is similar to coreference treatment, but here the words are directly implemented in the table (not numeric values)\n",
    "\n",
    "**How**: The collapsed-ccprocessed dependencies tag are used. It consist of the third level of dependencies (the first and second one are steps, and each of them is processed by the NLP). All the governor and dependant words are associated, and the type of gramatical relationship is also saved. To achieve this, a loop is first done on each dependance that are further appended to a global dataframe for a single summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependence_treatment(Bs_data):\n",
    "\n",
    "#cc-processed reference\n",
    "    dependence_by_summary = pd.DataFrame(columns=['type','gov','dep'],dtype='object')\n",
    "    dependence = Bs_data.find_all('collapsed-ccprocessed-dependencies')\n",
    "    \n",
    "    #loop on each dependence (collapsed) tag\n",
    "    for dep_index in np.arange(0,len(dependence)):\n",
    "        \n",
    "        #get the dependent words and the govenor words, the dependency type matrix is created but not filled\n",
    "        linked_words_attr = np.zeros([len(dependence[dep_index].find_all('dep')),1],dtype=\"object\")\n",
    "        linked_words_gov = pd.DataFrame(data=np.array(dependence[dep_index].find_all(\"governor\")),dtype=\"object\")\n",
    "        linked_words_dep = pd.DataFrame(data=np.array(dependence[dep_index].find_all(\"dependent\")),dtype=\"object\")\n",
    "        \n",
    "        #filling of the dependency type using a loop\n",
    "        for attribute_index in np.arange(0,len(dependence[dep_index].find_all(\"dep\"))):\n",
    "            linked_words_attr[attribute_index] = dependence[dep_index].find_all(\"dep\")[attribute_index].attrs['type']\n",
    "            \n",
    "        linked_words_attr = pd.DataFrame(data=linked_words_attr,dtype='object')\n",
    "        \n",
    "        #concatenation of each table into a single one\n",
    "        dependence_table = pd.concat([linked_words_attr,linked_words_gov,linked_words_dep],axis=1)\n",
    "        dependence_table=dependence_table.set_axis(['type','gov','dep'],axis=1)\n",
    "        \n",
    "        #save the result\n",
    "        dependence_by_summary=dependence_by_summary.append(dependence_table,ignore_index=True)\n",
    "        \n",
    "    return dependence_by_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c7ccc",
   "metadata": {},
   "source": [
    "### Sentence treatment\n",
    "\n",
    "**Goal**: The complex-to-understand function has multiple roles. The main goal is to analyze each sentence of the summary and to retrieve any character name defined as a \"Person\" object by the NLP. It create a dataframe where each row is a sentence, with all the Lemmas of the sentence and also grammatical characteristic lemmas regrouped (verb, nouns, etc). The second goal is to create a matrix (similar to a BOW) containing each sentence by rows and each words of the sentences by columns.\n",
    "\n",
    "**How**: First a loop is computed on each sentence of a summary. Then, for each sentence, all the words, the grammatical structure type, the object type and the lemmas is retrieved. The \"Person\" object are found and further processed: If a person have a first and second name that are subsequent, the function record it as a single person (eg. Harry Potter), this is done by comparing the grammatical type of subsequent \"Persons\" placement in the sentence (if there is a \"Person\" at word_id=11 and word_id=12). Then the function also handle the fact that there can be multiple characters in one sentence and thus create a single string to save the characters name (Harry Potter & Hagrid). If no character is found, it simply add \"NoCharacter\" in the character column. Finally, the words of a certain gramatical type are grouped together to be saved in a single cell depending of the word type.\n",
    "\n",
    "\n",
    "**Comment**: This the messy function as it was the first created and the architecture works, but has requiered adjustement all along the script creation. It can be easily seen that the python/xml file handling was not of high performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc52f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The complex-to-understand part, here the goal is to concatenate each sentence word and to extract a potential character in the sentence\n",
    "\n",
    "\n",
    "def sentence_treatment(Bs_data):\n",
    "    \n",
    "    #compute the number of sentence in a summary\n",
    "    number_of_sentence = len(Bs_data.find_all('sentence',id=True))\n",
    "\n",
    "    #initialization of the variable which will store the results\n",
    "    all_sentence = pd.DataFrame(columns=[\"sentence_id\",\"Lemma\",\"Characters\",\"multiple_character\",\"Full_sentence\"],dtype=\"object\")\n",
    "\n",
    "    #the sentence matrix is used to have each sentence by row and each word by column\n",
    "    sentence_matrix= np.zeros([500,500],dtype=\"object\")\n",
    "    sentence_matrix.fill(\"NaN\")\n",
    "    \n",
    "    #dataframe to save for each sentences, words of a specific type: ex adjective\n",
    "    words_type_saving = pd.DataFrame(columns=np.array([\"sentence_id\",\"JJ\",\"NN\",\"NNP\",\"NNS\",\"PRP\",\"PRP$\",\"VB\",\"VBG\",\"VBP\",\"VBZ\"]),dtype='object')\n",
    "\n",
    "\n",
    "    #loop on each sentence of the summary\n",
    "    for index in range(number_of_sentence):\n",
    "    \n",
    "         #select sentence number [index]\n",
    "        sentence = Bs_data.find_all('sentence')[index]\n",
    "    \n",
    "        #create array of each useful variable : \n",
    "        # - The sentence's words\n",
    "        # - The kind of grammatical structure (POS), ex: subject, verb, pronouns, etc\n",
    "        # - The object type (NER), ex: DATE, PERSON, ORGANIZATION, etc\n",
    "        # - The associated lemma (for each word): beeing => be\n",
    "\n",
    "        words = np.array(sentence.find_all(\"word\"))\n",
    "        pos = np.array(sentence.find_all(\"POS\"))\n",
    "        ner = np.array(sentence.find_all(\"NER\"))\n",
    "        lemma = np.array(sentence.find_all(\"lemma\"))\n",
    "    \n",
    "        #concatenate all those variable in an array\n",
    "        con = np.concatenate([words,lemma,pos,ner],axis=1)\n",
    "    \n",
    "        #create a dataframe for comodity\n",
    "        temp_sentence = pd.DataFrame(data=con,columns=[\"Word\",\"Lemma\",\"Grammar\",\"Object\"])\n",
    "        # add an index for each sentence in one summary\n",
    "        temp_sentence[\"sentence_id\"] = index\n",
    "        # as in the file the word's index start from 1 (for coreferences), the creation of this index is used after\n",
    "        temp_sentence[\"Idx\"] = temp_sentence.index +1\n",
    "\n",
    "        #select the potential characters in a sentence, \n",
    "        characters = temp_sentence[temp_sentence.Object == \"PERSON\"]\n",
    "\n",
    "        #tricks to put together characters name made of multiple word: ex Arthur Lambert\n",
    "        # creation of a second index for words with a increased step of 1\n",
    "        idx2 = characters.Idx.values +1 \n",
    "        charac2 = characters.copy()\n",
    "        charac2.Idx = idx2\n",
    "\n",
    "        #merge the doubled character dataframe with a difference of one in word index: Thus, each characters name that are \n",
    "        # spread in two words are linked together and merged\n",
    "        merged_characters_for_people_with_forname_and_last_name = pd.merge(characters,charac2,on=\"Idx\")\n",
    "    \n",
    "        #------------------\n",
    "        #Here begins a set of conditions to determine what sort of name the NLP has found:\n",
    "        # - only characters with a forname\n",
    "        # - characters with forname and lastname\n",
    "        # - multiple characters\n",
    "        # And also the string work to have readable characters name\n",
    "        \n",
    "        only_forname = False\n",
    "        \n",
    "        # As the merge occurs for successive words, if the characters only have a single-word name, the merge table length is 0\n",
    "        # Also, it is ensured with the second condition that there is some characters found previously (in characters dataframe)\n",
    "        if len(merged_characters_for_people_with_forname_and_last_name)==0 and len(characters) > 1:\n",
    "            only_forname = True\n",
    "    \n",
    "        # multiple characters\n",
    "        if (len(characters)>1 | only_forname):\n",
    "            \n",
    "            # multiple characters with mulitple words: ex Duke Henry\n",
    "            if len(merged_characters_for_people_with_forname_and_last_name)!=0:\n",
    "                \n",
    "                # Merging \"Duke\" + \" \" + \"Henry\"\n",
    "                full_name = merged_characters_for_people_with_forname_and_last_name.Word_y +\\\n",
    "                \" \" + merged_characters_for_people_with_forname_and_last_name.Word_x\n",
    "                \n",
    "                # Merging the grammatical object to ensure next that it correspond to names: ex not \"Henry\" + \" \" + \"and\"\n",
    "                both_grammar = merged_characters_for_people_with_forname_and_last_name.Grammar_x + \\\n",
    "                merged_characters_for_people_with_forname_and_last_name.Grammar_y\n",
    "                \n",
    "                #condition computation to get full names :  \"Henry\" + \" \" + \"and\" would be \"NNPAND\"\n",
    "                both_subjects_words = (both_grammar == \"NNPNNP\")\n",
    "                \n",
    "                #selection of full names (only Subject words) into the dataframe of potential full names\n",
    "                full_name=full_name[both_subjects_words]\n",
    "                \n",
    "                #String manipulation to get each full names separeted by \"&\"\n",
    "                sentence_characters = str(full_name.values).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\" \").replace(\"  \",\" &\")\n",
    "            #multiple characters with  no composed/full names\n",
    "            else:\n",
    "                \n",
    "                # if there is more than one characters, just ensure there is no duplicate\n",
    "                if len(characters)>1:\n",
    "                    characters = characters[characters.Word.duplicated(keep='first')==False]\n",
    "                \n",
    "                # find characters with only fornames\n",
    "                characters_list = str(characters.Word.values)\n",
    "  \n",
    "                # String concatenation\n",
    "                sentence_characters = characters_list.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\" \").replace(\"  \",\" &\")\n",
    "            \n",
    "            #condition about NLP not find a characters name even if there is one (Object != PERSON | ORGA)\n",
    "            if len(sentence_characters) ==0:\n",
    "                sentence_characters =  \"NoCharacter\"\n",
    "        else:\n",
    "            # normally only one character\n",
    "            characters_list = str(characters.Word.values)\n",
    "        \n",
    "            sentence_characters = characters_list.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\" \").replace(\"  \",\" &\")\n",
    "        \n",
    "    \n",
    "        # last condition to ensure that if there is no characters, the \"NoCharacter\" is assigned\n",
    "        if len(characters) == 0:\n",
    "            sentence_characters = \"NoCharacter\"\n",
    "\n",
    "\n",
    "        # get sorted each type of words\n",
    "        words_type = pd.DataFrame(data=np.concatenate([words,pos],axis=1),columns=[\"Words\",\"Type\"],dtype='object')\n",
    "        #group the words of each type by type\n",
    "        words_type_grouped = words_type.groupby('Type')['Words'].apply(', '.join).reset_index()\n",
    "        #some manipulation to get a dataframe with type as columns\n",
    "        words_type_grouped=words_type_grouped.set_index(words_type_grouped.Type).transpose().drop(labels=\"Type\",axis=0)\n",
    "        \n",
    "        #save result in general table (thus for each sentences)\n",
    "        words_type_saving = words_type_saving.append(words_type_grouped)\n",
    "        words_type_saving[\"sentence_id\"].iloc[index] = index\n",
    "        \n",
    "        #recreate the full sentence in single string (for conveniance while testing the script)\n",
    "        full_sentence = temp_sentence.Word\n",
    "        final_sentence = \"\"\n",
    "        for word in full_sentence:\n",
    "            final_sentence += \" \" +word\n",
    "    \n",
    "    \n",
    "        # Join the lemma of each sentence in a single string\n",
    "        temp_sentence = temp_sentence.groupby('sentence_id')['Lemma'].apply(', '.join).reset_index()\n",
    "        \n",
    "        #assigne characters to a sentence\n",
    "        temp_sentence['Characters'] = sentence_characters\n",
    "        test =temp_sentence     \n",
    "        \n",
    "        #assign the full sentence (only used for commodity while doing the algorithm)\n",
    "        temp_sentence['Full_sentence'] = final_sentence\n",
    "    \n",
    "    \n",
    "        #computing the sentence matrix which contain each sentence on a row and each word of this sentence sorted in columns\n",
    "        sentence_matrix[index,:len(words)] = words.reshape([1,len(words)])\n",
    "    \n",
    "    \n",
    "\n",
    "        #append the result of one sentence to the other ones of a same summary\n",
    "        all_sentence = all_sentence.append(temp_sentence)\n",
    "        \n",
    "        #append the words of specific types\n",
    "    all_sentence = pd.merge(all_sentence,words_type_saving,on=\"sentence_id\")\n",
    "        \n",
    "        \n",
    "    return all_sentence,sentence_matrix,test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e24fb",
   "metadata": {},
   "source": [
    "### Link Words\n",
    "\n",
    "**Goal**: From the coreference relation table (numeric) create a table of actual words (string). The coreference that do not contain any character name (based on upper case letter) are droppped.\n",
    "\n",
    "**How**: Use the relation table with sentence number and word placement into the sentence number combined to the senence matrix (row = sentence, column = word placement) to create the completed links sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d05c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreate the links \n",
    "\n",
    "def link_words(all_link,sentence_matrix):\n",
    "\n",
    "# initialization\n",
    "    all_link_completed = all_link.copy()\n",
    "    all_link_completed[\"rep_name\"] = \"NaN\"\n",
    "    all_link_completed[\"dep_name\"] = \"NaN\"\n",
    "\n",
    "    #attribue for each link the associated words\n",
    "    for row in range(len(all_link)):\n",
    "        all_link_completed[\"rep_name\"].iloc[row] = sentence_matrix[int(all_link[\"representative_sentence\"].iloc[row])-1,int(all_link[\"representative_head\"].iloc[row])-1]\n",
    "        all_link_completed[\"dep_name\"].iloc[row] = sentence_matrix[int(all_link[\"dep_sentence\"].iloc[row])-1,int(all_link[\"dep_head\"].iloc[row])-1]\n",
    "\n",
    "    \n",
    "    #remove rows with same representative and dependant words\n",
    "    all_link_completed = all_link_completed[all_link_completed.rep_name != all_link_completed.dep_name]\n",
    "\n",
    "    #remove rows where the representative does not start with an uppercase letter (= \"Nom Propre\")\n",
    "    to_drop = np.full([len(all_link_completed),1],-1)\n",
    "    \n",
    "    #loop to save links with reference to some names\n",
    "    for index in range(len(all_link_completed)):\n",
    "    \n",
    "        #condition based on character name starting with upper case letter, thus dropping links where no names appears as representative\n",
    "        row_not_abandonned=0 #variable just used to check performance\n",
    "        if str(all_link_completed.rep_name.iloc[index])[0].isupper():\n",
    "            row_not_abandonned +=1\n",
    "        \n",
    "        else:\n",
    "            to_drop[index] = all_link_completed.index[index]\n",
    "        \n",
    "    #drop links without characters names\n",
    "    all_link_completed= all_link_completed.drop(np.unique(to_drop)[1:])  \n",
    "    \n",
    "    return all_link_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90a989",
   "metadata": {},
   "source": [
    "### Character assignment to sentence without any explicit character name found using the coreference\n",
    "\n",
    "**Goal**: When a sentence has no character (\"NoCharacter\"), the function looks into the coreference links completed (string, see above) to detect if the sentence is linked to another one where a character has been found\n",
    "\n",
    "**How**: Loop on sentence without character, find the corresponding sentence in the coreference links, if there is one, assign the character name to the sentence, otherwise assign \"NotFound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb386c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_characters_to_sentence_without_explicit_character_name(all_sentence,all_link_completed):\n",
    "    # this part consist of attributing character names to sentence where no character was found by the NLP (thus, no PERSON object)\n",
    "    all_sentence_completed = all_sentence.copy()\n",
    "    \n",
    "    #loop on sentences without character: denoted \"NoCharacter\" as character name\n",
    "    for no_charac_sentence in all_sentence[all_sentence.Characters == \"NoCharacter\"].sentence_id:\n",
    "        \n",
    "        #get the character name from the completed links table\n",
    "        rep=all_link_completed[all_link_completed.dep_sentence == str(no_charac_sentence+1)].rep_name\n",
    "    \n",
    "    \n",
    "        #if no character name was found in links before, \"NotFound\" is set as \"character name\" to this sentence\n",
    "        if rep.empty:\n",
    "            all_sentence_completed.iloc[no_charac_sentence].Characters = \"NotFound\"\n",
    "        \n",
    "        #otherwise the character name is inserted in table\n",
    "        else:\n",
    "            all_sentence_completed.iloc[no_charac_sentence].Characters =rep.values[0]\n",
    "            \n",
    "    return all_sentence_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecac99",
   "metadata": {},
   "source": [
    "## Assign gender from characters_metadata\n",
    "\n",
    "\n",
    "**Goal**: Search for correspondances in character name found in the summary and character name in the character metadata (which regroup also the actors characteristics). If the name match, the gender of the actor is associated to the character from the summary.\n",
    "\n",
    "**How**: First lists of words linked to a gender, or recurrent undefined (in term of gender) words, are created. These came from the first 1500 lines of the character metadata and was manually done. Next, the involved character are obtained by a match on the film id. Some work is done on both the involved character (from metadata) and the character from a summary. It consists of lowercasing each letter and to remove the potential undefined words in each character name (There was a lot of lost due to \"Insp. Ron\" beeing just \"Ron\" or \"Inspector Ron\" respectively in metadata or summaries. \n",
    "In a second time, a loop on each character and nested loop if there is multiple character is performed. First a direct match between the reworked character name is done. If it don't match, None is applied to gender and more conditions are applied:\n",
    "- if the first word of the character contains a typical male name (eg Mr.)\n",
    "- if the first word of the character contains a typicale female name (eg Mrs.)\n",
    "- if the first word of the character name match with any of the metadata words (thus from Harry, if in the metadata it is registered as Harry Potter, it will match). This condition can be problematic if there is some undefined words not registered and if some characters of a same film have the same surname)\n",
    "The gender are obtained if a match occurs and assigned to characters in list form\n",
    "\n",
    "\n",
    "**Commment**: This is the only function that is not directly in the main, in this part of the \"group_by_character\" function (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "867e6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_gender_from_characters_metadata(all_sentence_completed,characters_data,film_id):\n",
    "    condition_to_look_into_metadata = 1\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------------------------\n",
    "    #the lists are based on character_name data 1500 first lines\n",
    "\n",
    "    male_words = np.array([\"dad\",\"monsieur\",\"mr\",\"mr.\",\"boy\",\"man\",\"king\",\"prince\",\"emperor\",\"father\",\"brother\",'sir'])\n",
    "    female_words = np.array([\"mom\",\"madame\",\"mrs\",\"mrs.\",\"ms\",\"ms.\",\"miss\",\"woman\",\"girl\",\"queen\",\"princess\",\"empress\",\"nurse\"\\\n",
    "                    ,\"mother\",\"lady\",\"sister\",\"wife\"])\n",
    "    undefined_words = np.array([\"lieutenant\",\"sgt\",\"sgt.\",\"commander\",\"fbi\",\"profiler\",\"boss\",\"detective\",\"dr\",\"dr.\",\"doc\",\"duke\"\\\n",
    "                   ,\"young\",\"doctor\",\"capt.\",\"captain\",\"jr.\",\"judge\",\"reverend\",\"major\",\"principal\",\"professeur\"\\\n",
    "                   ,\"professor\",\"inspector\",\"general\",\"lt.\",\"officer\",\"student\",\"president\",\"insp.\"])\n",
    "    #-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #retrieve the characters from the character metadata using the film id\n",
    "    involved_characters = characters_data[characters_data[\"Wikipedia_movie_ID\"]==int(film_id)]\n",
    "    \n",
    "    #if the metadata dataframe is empty just return to main \n",
    "    if involved_characters.empty:\n",
    "        return all_sentence_completed\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------------------------------------\n",
    "    #involved character pre-treatment\n",
    "    if (all(pd.isna(involved_characters.Character_name))):\n",
    "        condition_to_look_into_metadata = 0\n",
    "        \n",
    "    #split the characters name (if multiple words are composing it)\n",
    "    inv_charac_name = involved_characters.Character_name.str.lower().str.split(\" \",expand=True)\n",
    "    #insert NaN for words in undefined_words\n",
    "    inv_charac_name[inv_charac_name.isin(undefined_words)] = None\n",
    "    #harmonisation of the NaN type => None (for object)\n",
    "    inv_charac_name[inv_charac_name.isna()]=None\n",
    "    #recreate a single string from characters (potential) multiple names\n",
    "    inv_charac_name=pd.DataFrame(inv_charac_name.stack().reset_index())\n",
    "    inv_charac_name = inv_charac_name.groupby(inv_charac_name.level_0)[0].apply(\" \".join)\n",
    "    #join the result to the initial table\n",
    "    involved_characters=involved_characters.join(inv_charac_name,on=involved_characters.index).rename(columns={0:\"Character_name_reworked\"})\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------------------------------------\n",
    "    #character found in summaries pretreatment for comparison with character_metadata\n",
    "    \n",
    "    #same as above function for involved character but with a loop to tackle multiple characters\n",
    "    summary_charac=pd.DataFrame(all_sentence_completed.Characters.tolist(),dtype=\"object\")\n",
    "    column_to_drop =  np.arange(summary_charac.shape[1])\n",
    "    for columns in summary_charac.columns:\n",
    "        charac = summary_charac[columns].str.lower().str.split(\" \",expand=True)\n",
    "        charac[charac.isin(undefined_words)] = None\n",
    "        charac[charac.isna()]=None\n",
    "        charac=pd.DataFrame(charac.stack().reset_index())\n",
    "        charac = charac.groupby(charac.level_0)[0].apply(\" \".join)\n",
    "        charac.name = \"reworked_\" + str(columns) \n",
    "        summary_charac=summary_charac.join(charac,on=summary_charac.index)\n",
    "\n",
    "    summary_charac=summary_charac.drop(columns=column_to_drop)\n",
    "    #-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    all_sentence_completed[\"Gender\"] = 'NaN'\n",
    "\n",
    "    i=0\n",
    "    #loop on each row of the summary characters\n",
    "    for characters in all_sentence_completed.Characters:\n",
    "        \n",
    "        j=0\n",
    "        number_of_character = len(characters)\n",
    "\n",
    "        character_gender_list = []\n",
    "        #loop on each characters of a row (if many essentially, else it is a single way loop)\n",
    "        for single_character_number in range(number_of_character):\n",
    "            single_character=characters[single_character_number]\n",
    "         \n",
    "            #-------------------------------------------------------------------------------------------------------------------------------\n",
    "            #try a direct match of reworked character names\n",
    "            if (involved_characters[involved_characters.Character_name_reworked == summary_charac.iloc[i,j]].empty == False):\n",
    "                character_gender = list(involved_characters[involved_characters.Character_name_reworked == summary_charac.iloc[i,j]].Actor_gender)\n",
    "            else:\n",
    "                character_gender = None\n",
    "            #-------------------------------------------------------------------------------------------------------------------------------                     \n",
    "            \n",
    "            # if no direct match, then alternatives are tried\n",
    "            if (character_gender == None) & (isinstance(summary_charac.iloc[i,j],str)):\n",
    " \n",
    "                # the first part of name is determinant, as it is either a words linked to a male or female (eg. Mr.)\n",
    "                name_split = summary_charac.iloc[i,j].strip().split(\" \")\n",
    "                \n",
    "                #find if male word implied\n",
    "                if any(pd.Series(name_split[0]).isin(male_words)):\n",
    "                    character_gender = list(\"M\")\n",
    "                    \n",
    "                #find if female word implied\n",
    "                elif any(pd.Series(name_split[0]).isin(female_words)):\n",
    "                    character_gender = list(\"F\")\n",
    "                \n",
    "                #look if the first word of the name (surname) is in the character list (e.g Harry in summaries is Harry Potter in metadata, this will match) \n",
    "                elif (condition_to_look_into_metadata == 1)  &  (isinstance(involved_characters.Character_name_reworked,str)):\n",
    "                    if (any(name_split[0] ==  involved_characters.Character_name_reworked.str.lower().str.split(\" \",expand=True).iloc[:,0])):\n",
    "                        character_gender=list(involved_characters[involved_characters.Character_name_reworked.str.lower().str.split(\" \",expand=True).iloc[:,0] == name_split[0]].Actor_gender)\n",
    "                    \n",
    "                \n",
    "                #-------------------------------------------------------------------------------------------------------------------------------\n",
    "            character_gender_list.append(character_gender)\n",
    "            j+=1\n",
    "\n",
    "        all_sentence_completed.Gender.iloc[i] = character_gender_list\n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return all_sentence_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea258b",
   "metadata": {},
   "source": [
    "### Grouping on characters \n",
    "\n",
    "**Goal**: The idea is to regroup each sentences around a specific character or set of characters. Thus, if there is 3 sentences (3 rows at start) with the same character, it results in one row with all the characteristics (lemmas, specific lemmas (eg. nouns, verb) and governor/dependent words). It also launch the gender identification.\n",
    "\n",
    "**How**: It is mainly an excessive use of a groupby(characters) completed with a .apply(','.join) function. it has to be done independently for each type of variable that needs to be grouped and all the subsets are then merge in one table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e7837d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_characters(all_sentence_completed,all_summary_characters,dependence_by_summary,characters_data):\n",
    "    \n",
    "    #remove useless spaces: such as \" Marie \" == \"Marie\" will be true\n",
    "    all_sentence_completed.Characters= all_sentence_completed.Characters.str.strip()\n",
    "\n",
    "    #Used for computing and merging the grouped words of each type by characters to the following dataframe\n",
    "    all_sentence_completed_copy = all_sentence_completed.copy()\n",
    "\n",
    "    #Finally, the regroupement of all the sentences  to unique characters for each summary \n",
    "    \n",
    "    all_sentence_completed = all_sentence_completed.groupby('Characters')['Lemma'].apply(','.join).reset_index()\n",
    "    all_sentence_completed['Lemma'] = all_sentence_completed['Lemma'].str.split(\",\").apply(list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #type_final = all_sentence_completed_copy.groupby('Characters')['JJ'].apply(', '.join).reset_index()\n",
    "    type_to_keep = np.array([\"JJ\",\"NN\",\"NNP\",\"NNS\",\"PRP\",\"PRP$\",\"VB\",\"VBG\",\"VBP\",\"VBZ\"])\n",
    "    \n",
    "    #group words of each type in a single cell by characters\n",
    "    all_sentence_completed_copy = all_sentence_completed_copy.fillna(\"\")\n",
    "    for type_name in type_to_keep:\n",
    "        \n",
    "        type_final = all_sentence_completed_copy.groupby('Characters')[type_name].apply(', '.join).reset_index()\n",
    "        type_final[type_name] = type_final[type_name].str.split(\",\").apply(list)\n",
    "        all_sentence_completed = pd.merge(all_sentence_completed,type_final, on=\"Characters\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    #working on the collapsed dependence\n",
    "    #find dependence implying character names\n",
    "    dep_words_by_character = dependence_by_summary[dependence_by_summary[\"gov\"]\\\n",
    "                                                   .isin(all_sentence_completed.Characters.unique())]\\\n",
    "                                                   .groupby('gov')[\"dep\"].apply(', '.join).reset_index()\n",
    "    dep_words_by_character.columns=[\"Characters\",\"dependent_words\"]\n",
    "    #create a list the words neglecting their type of relation with the character name\n",
    "    if (dep_words_by_character[\"dependent_words\"].isnull().all() == False):\n",
    "        dep_words_by_character[\"dependent_words\"] = dep_words_by_character[\"dependent_words\"].str.split(\",\").apply(list)\n",
    "    \n",
    "    #same as above but for the governor words   \n",
    "    gov_words_by_character = dependence_by_summary[dependence_by_summary[\"dep\"]\\\n",
    "                                                   .isin(all_sentence_completed.Characters.unique())]\\\n",
    "                                                   .groupby('dep')[\"gov\"].apply(', '.join).reset_index()\n",
    "    gov_words_by_character.columns=[\"Characters\",\"governor_words\"]\n",
    "    if (gov_words_by_character[\"governor_words\"].isnull().all() == False):\n",
    "        gov_words_by_character[\"governor_words\"] = gov_words_by_character[\"governor_words\"].str.split(\",\").apply(list)\n",
    "    \n",
    "    #join the lists of words to the characters in the global table\n",
    "    all_sentence_completed = all_sentence_completed.join(gov_words_by_character.set_index('Characters'),on=\"Characters\",how='left')\n",
    "    all_sentence_completed = all_sentence_completed.join(dep_words_by_character.set_index('Characters'),on=\"Characters\",how='left')\n",
    "    \n",
    "    #create a list of character rather than a simple string (mainly if there is multiple character)\n",
    "    all_sentence_completed[\"Characters\"] = all_sentence_completed[\"Characters\"].str.split(\"&\").apply(list)\n",
    "    \n",
    "    \n",
    "    #adding a column with the film ID\n",
    "    film_id = path[len(\"data/corenlp_plot_summaries/\"):].replace(\".txt.xml\",\"\")\n",
    "    all_sentence_completed[\"film_id\"] = film_id\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #launche the function to seek for characters' gender\n",
    "    all_sentence_completed_2 = assign_gender_from_characters_metadata(all_sentence_completed,characters_data,film_id)\n",
    "    \n",
    "  \n",
    "    #save result in the new dataset\n",
    "    all_summary = all_summary_by_characters.append(all_sentence_completed_2)\n",
    "    \n",
    "    \n",
    "    return all_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c1559",
   "metadata": {},
   "source": [
    "# MAIN\n",
    "\n",
    "The main function/script is a simple loop on each summaries, it applies the above function in the good order and result in a single dataframe containing all the summaries lemmas, specific words, gov/dep words and gender based on each characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca904b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 7/42306 [00:06<11:55:53,  1.02s/it]C:\\Users\\saiht\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "  0%|                                                                             | 35/42306 [00:24<7:46:33,  1.51it/s]C:\\Users\\saiht\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:866: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.array(value)\n",
      " 23%|████████████████▋                                                       | 9829/42306 [2:45:56<10:15:44,  1.14s/it]"
     ]
    }
   ],
   "source": [
    "all_summary_by_characters = pd.DataFrame(columns=[\"film_id\",\"Characters\",\"Lemma\"],dtype=\"object\")\n",
    "\n",
    "summary_index = 0\n",
    "\n",
    "# assign directory\n",
    "directory = 'data/corenlp_plot_summaries'\n",
    "\n",
    "#open characters data\n",
    "path_charac = 'data\\MovieSummaries\\character.metadata.tsv'\n",
    "colnames=['Wikipedia_movie_ID', 'Freebase_movie_ID', 'Movie_release_date', 'Character_name','Actor_date_of_birth', 'Actor_gender', 'Actor_height', 'Actor_ethnicity', 'Actor_name', 'Actor_age_at_movie_release', 'Freebase_character/actor_map_ID', 'Freebase_character_ID', 'Freebase_actor_ID'] \n",
    "characters_raw_data = pd.read_csv(path_charac, sep='\\t',names = colnames, header=None)\n",
    "\n",
    "characters_data = characters_raw_data.copy()\n",
    "\n",
    "\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    path = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(path):\n",
    "        last_summary = path\n",
    "    else:\n",
    "        print(\"error in file reading\")\n",
    "        \n",
    "        \n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "        \n",
    "        \n",
    "        Bs_data = BeautifulSoup(data, \"xml\")\n",
    "        \n",
    "        #coreference treatment\n",
    "        all_link = coreference_treatment(Bs_data)\n",
    "        \n",
    "        #dependence treatment\n",
    "        dependence_by_summary = dependence_treatment(Bs_data)\n",
    "        \n",
    "        \n",
    "        #sentences treatment\n",
    "        all_sentence,sentence_matrix,test= sentence_treatment(Bs_data)\n",
    "        \n",
    "        \n",
    "        #coreference from numerical links to words linked dataframe\n",
    "        all_link_completed=link_words(all_link,sentence_matrix)\n",
    "        \n",
    "        \n",
    "        #for sentences with no characters, use coreferences to assign a character\n",
    "        all_sentence_completed=assign_characters_to_sentence_without_explicit_character_name(all_sentence,all_link_completed)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        all_summary_by_characters = group_by_characters(all_sentence_completed,all_summary_by_characters,dependence_by_summary,characters_data)\n",
    "        \n",
    "        \n",
    "    summary_index+=1\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to pickle\n",
    "all_summary_by_characters.to_pickle(\"data\\summary_final.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
